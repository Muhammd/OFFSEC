<!DOCTYPE html>
<html lang="en" class="m-0 p-0 w-screen min-h-screen bg-navy-dark"><head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Adversarial Attack with FGSM (Fast Gradient Signed Method) | OffSec Wiki</title>
        <meta name="description" content="Adversarial Attack is the method to fool a neural network. This leads misclassification of a classification model. The FGSM attack is also known as white-box attack. In short, we need to know about the model’s architecture to achieve this attack">
        
        <link rel="icon" type="image/x-icon" href="/img/favicon-32.png">
        <link rel="stylesheet" href="/pagefind/pagefind-ui.css"><link rel="stylesheet" href="/styles.css">
        <link rel="stylesheet" href="/css/carbonads.css">
        
        <script type="text/javascript" src="/js/exploit.js"></script>
        

        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@muhammdhaidari">
        <meta name="twitter:creator" content="@muhammdhaidari">
        <meta property="og:url" content="https://offsec.PenTest.tools/exploit/machine-learning/computer-vision/adversarial-attack-with-fgsm/">
        <meta property="og:title" content="Adversarial Attack with FGSM (Fast Gradient Signed Method) | OffSec Wiki">
        <meta property="og:description" content="Adversarial Attack is the method to fool a neural network. This leads misclassification of a classification model. The FGSM attack is also known as white-box attack. In short, we need to know about the model’s architecture to achieve this attack">
        <meta property="og:image" content="https://offsec.PenTest.tools/img/screenshot.png">
        
        <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-RR6XYHYTQ9"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-RR6XYHYTQ9');
        </script>
    <script type="text/javascript" src="/pagefind/pagefind-ui.js"></script><script type="text/javascript">window.addEventListener('DOMContentLoaded', () => { new PagefindUI({"element":"#search","showImages":false,"showEmptyFilters":true,"resetStyles":true,"bundlePath":"/pagefind/","baseUrl":"/"}); });</script><link rel="stylesheet" href="/codecopy/codecopy.css"><script type="text/javascript" src="/codecopy/codecopy.js"></script></head>

    <body class="m-0 p-0 w-screen min-h-screen text-basic text-white text-sm">

        <header class="sticky top-0 w-full h-[70px] z-20 bg-gradient-to-r from-pink-dark">
    <div class="mx-auto w-full sm:w-2/3 h-full flex justify-around items-center">
        <div class="w-full h-full p-2 flex items-center space-x-3">
            <a href="/" class="hidden sm:block">
                <span class="text-base text-white font-bold">OFFSEC Wiki</span>
            </a>
            
            <div id="search"></div>
        </div>

        
        <div class="hidden sm:flex w-full h-full p-2 items-center justify-center space-x-3">
            
            <a href="https://github.com/muhammd/" target="_blank" rel="noopener noreferrer" alt="GitHub">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="fill-white w-8 h-8 hover:brightness-200">
                    <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"></path></svg>
            </a>
            
            <a href="https://twitter.com/muhammdhaidari" target="_blank" rel="noopener noreferrer" alt="Twitter">
                <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewbox="0 0 24 24" class="fill-sky w-8 h-8 hover:brightness-200">
                    <path d="M12 0c-6.627 0-12 5.373-12 12s5.373 12 12 12 12-5.373 12-12-5.373-12-12-12zm6.066 9.645c.183 4.04-2.83 8.544-8.164 8.544-1.622 0-3.131-.476-4.402-1.291 1.524.18 3.045-.244 4.252-1.189-1.256-.023-2.317-.854-2.684-1.995.451.086.895.061 1.298-.049-1.381-.278-2.335-1.522-2.304-2.853.388.215.83.344 1.301.359-1.279-.855-1.641-2.544-.889-3.835 1.416 1.738 3.533 2.881 5.92 3.001-.419-1.796.944-3.527 2.799-3.527.825 0 1.572.349 2.096.907.654-.128 1.27-.368 1.824-.697-.215.671-.67 1.233-1.263 1.589.581-.07 1.135-.224 1.649-.453-.384.578-.87 1.084-1.433 1.489z"></path></svg>
            </a>
        </div>
    </div>
</header>

<div id="search_bg" class="fixed top-0 left-0 w-full z-10 h-full bg-black opacity-50 hidden"></div>


<script>
    window.addEventListener('DOMContentLoaded', () => {
        //Display the black filter when searching
        const searchBgElem = document.getElementById("search_bg");
        const drawerElem = document.querySelector(".pagefind-ui__drawer");
        const inputElem = document.querySelector(".pagefind-ui__search-input");

        if (searchBgElem) {
            searchBgElem.addEventListener('click', () => {
                searchBgElem.classList.add("hidden");
                drawerElem.style.visibility = "hidden";
                drawerElem.style.opacity = 0;
            });
        }

        inputElem.addEventListener('focus', () => {
            if (searchBgElem) {
                searchBgElem.classList.remove("hidden");
                drawerElem.style.visibility = "visible";
                drawerElem.style.opacity = 1;
            }
        });
    });
</script>

        <main class="w-full h-full">
            <div class="block sm:grid grid-cols-[320px_minmax(400px,_1fr)_280px] py-2">

    
    <div id="exploit-leftside-inner" class="hidden sm:block w-full h-screen overflow-y-auto px-4 py-3">
        <div class="w-full">
            <div id="exploit-related">
                <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-60">
                    Exploits related to Machine Learning
                </h4>
                
                    <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-80">Computer Vision</h4>
                    
                        <div class="my-3">
                        
                            <span id="scroll-target" class="text-sm text-white">Adversarial Attack with FGSM (Fast Gradient Signed Method)</span>
                        
                        </div>
                    
                        <div class="my-3">
                        
                            <a href="/exploit/machine-learning/computer-vision/image-investigation/" class="text-sm text-pink-light hover:brightness-200">
                                Image Investigation
                            </a>
                        
                        </div>
                    
                        <div class="my-3">
                        
                            <a href="/exploit/machine-learning/computer-vision/image-manipulation/" class="text-sm text-pink-light hover:brightness-200">
                                Image Manipulation
                            </a>
                        
                        </div>
                    
                        <div class="my-3">
                        
                            <a href="/exploit/machine-learning/computer-vision/image-recognition-bypass/" class="text-sm text-pink-light hover:brightness-200">
                                Image Recognition Bypass
                            </a>
                        
                        </div>
                    

                    
                
                    <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-80">LLM</h4>
                    
                        <div class="my-3">
                        
                            <a href="/exploit/machine-learning/llm/llm-prompt-injection/" class="text-sm text-pink-light hover:brightness-200">
                                LLM Prompt Injection
                            </a>
                        
                        </div>
                    

                    
                
                    <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-80">Preprocessing</h4>
                    
                        <div class="my-3">
                        
                            <a href="/exploit/machine-learning/preprocessing/csv-data-manipulation/" class="text-sm text-pink-light hover:brightness-200">
                                CSV Data Manipulation
                            </a>
                        
                        </div>
                    
                        <div class="my-3">
                        
                            <a href="/exploit/machine-learning/preprocessing/dimensionality-reduction/" class="text-sm text-pink-light hover:brightness-200">
                                Dimensionality Reduction
                            </a>
                        
                        </div>
                    

                    
                
                    <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-80">Clustering</h4>
                    
                        <div class="my-3">
                        
                            <a href="/exploit/machine-learning/clustering/finding-optimal-number-of-clusters/" class="text-sm text-pink-light hover:brightness-200">
                                Finding Optimal Number of Clusters
                            </a>
                        
                        </div>
                    

                    
                
                    <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-80">Model</h4>
                    
                        <div class="my-3">
                        
                            <a href="/exploit/machine-learning/model/ml-model-investigation/" class="text-sm text-pink-light hover:brightness-200">
                                ML Model Investigation
                            </a>
                        
                        </div>
                    
                        <div class="my-3">
                        
                            <a href="/exploit/machine-learning/model/model-inversion-attack/" class="text-sm text-pink-light hover:brightness-200">
                                Model Inversion Attack
                            </a>
                        
                        </div>
                    

                    
                
                    <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-80">Others</h4>
                    

                    
                        
                            <div class="my-3">
                            
                                <a href="/exploit/machine-learning/jupyter-notebook-pentesting/" class="text-sm text-pink-light hover:brightness-200">
                                    Jupyter Notebook Pentesting
                                </a>
                            
                            </div>
                        
                            <div class="my-3">
                            
                                <a href="/exploit/machine-learning/orange-data-mining/" class="text-sm text-pink-light hover:brightness-200">
                                    Orange Data Mining
                                </a>
                            
                            </div>
                        
                            <div class="my-3">
                            
                                <a href="/exploit/machine-learning/read-hdf5-file/" class="text-sm text-pink-light hover:brightness-200">
                                    Read HDF5 (H5) File
                                </a>
                            
                            </div>
                        
                            <div class="my-3">
                            
                                <a href="/exploit/machine-learning/read-pt/" class="text-sm text-pink-light hover:brightness-200">
                                    Read PT File
                                </a>
                            
                            </div>
                        
                            <div class="my-3">
                            
                                <a href="/exploit/machine-learning/read-qasm/" class="text-sm text-pink-light hover:brightness-200">
                                    Read QASM
                                </a>
                            
                            </div>
                        
                    
                
            </div>
        </div>
    </div>
    

    <div id="exploit-center-inner" class="w-full h-screen overflow-y-auto px-6 py-3" data-pagefind-body="">
        <article>
            <div class="w-full p-4">
                <h1 id="exploit-title" class="text-4xl font-bold">Adversarial Attack with FGSM (Fast Gradient Signed Method)</h1>

                <p class="my-3 text-base">Last modified: 2023-08-21</p>
                <div class="my-3 flex flex-wrap items-end space-x-2 space-y-2">
                    
                    <span class="bg-pink-dark rounded-md text-base px-3 py-1 text-white">Computer Vision</span>
                    
                    <span class="bg-pink-dark rounded-md text-base px-3 py-1 text-white">Machine Learning</span>
                    
                </div>
            </div>

            <hr class="border-1 border-white opacity-40">

            <p class="px-2 py-4 text-base">Adversarial Attack is the method to fool a neural network. This leads misclassification of a classification model. The FGSM attack is also known as white-box attack. In short, we need to know about the model’s architecture to achieve this attack</p>

            <div id="exploit-content" class="px-2 py-4 text-base">
                <h2 id="create-adversarial-examples-against-resnet" tabindex="-1"><a href="#create-adversarial-examples-against-resnet" class="header-anchor">Create Adversarial Examples against ResNet</a></h2>
<p>Reference: <a href="https://pytorch.org/tutorials/beginner/fgsm_tutorial.html">PyTorch Docs</a></p>
<p>It's recommended to use an environment which is optimized to implement a machine learning model such as <strong>Google Colaboratory</strong>, <strong>Jupyter Notebook</strong>.</p>
<h3 id="1.-import-modules" tabindex="-1"><a href="#1.-import-modules" class="header-anchor">1. Import Modules</a></h3>
<pre class="codecopy_pre codecopy_pre_0"><code class="language-python hljs"><span class="syntax-keyword">import</span> torch
<span class="syntax-keyword">import</span> torch.nn <span class="syntax-keyword">as</span> nn
<span class="syntax-keyword">import</span> torch.nn.functional <span class="syntax-keyword">as</span> F
<span class="syntax-keyword">from</span> torchvision <span class="syntax-keyword">import</span> datasets, models, transforms
<span class="syntax-keyword">import</span> numpy <span class="syntax-keyword">as</span> np
<span class="syntax-keyword">from</span> PIL <span class="syntax-keyword">import</span> Image
</code><button class="codecopy_copy" data-codecopy="0">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="2.-load-resnet-model" tabindex="-1"><a href="#2.-load-resnet-model" class="header-anchor">2. Load ResNet Model</a></h3>
<p>We load the <strong>ResNet50</strong> pretrained on <strong>ImageNet</strong>. It's no problem whether <strong>ResNet18</strong>, <strong>ResNet34</strong>, etc.</p>
<pre class="codecopy_pre codecopy_pre_1"><code class="language-python hljs">model = models.resnet50(pretrained=<span class="syntax-literal">True</span>)
model.<span class="syntax-built_in">eval</span>()

torch.manual_seed(<span class="syntax-number">42</span>)
use_cuda = <span class="syntax-literal">True</span>
device = <span class="syntax-string">"cuda"</span> <span class="syntax-keyword">if</span> torch.cuda.is_available() <span class="syntax-keyword">else</span> <span class="syntax-string">"cpu"</span>
<span class="syntax-built_in">print</span>(<span class="syntax-string">"Device: "</span>, device)
</code><button class="codecopy_copy" data-codecopy="1">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="3.-load%2Fpreprocess-image" tabindex="-1"><a href="#3.-load%2Fpreprocess-image" class="header-anchor">3. Load/Preprocess Image</a></h3>
<p>We use the image of the fluffy samoyed dog.</p>
<pre class="codecopy_pre codecopy_pre_2"><code class="language-python hljs">wget https://github.com/pytorch/hub/raw/master/images/dog.jpg
</code><button class="codecopy_copy" data-codecopy="2">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>Then need to preprocess it.</p>
<pre class="codecopy_pre codecopy_pre_3"><code class="language-python hljs"><span class="syntax-comment"># Define a function which preprocesss the original image</span>
preprocess = transforms.Compose([
  transforms.Resize(<span class="syntax-number">256</span>),
  transforms.CenterCrop(<span class="syntax-number">224</span>),
  transforms.ToTensor(),
  transforms.Normalize(mean=[<span class="syntax-number">0.485</span>, <span class="syntax-number">0.456</span>, <span class="syntax-number">0.406</span>], std=[<span class="syntax-number">0.229</span>, <span class="syntax-number">0.224</span>, <span class="syntax-number">0.225</span>]),
])

orig_img_tensor = preprocess(orig_img)

<span class="syntax-comment"># Prepend one dimension to the tensor for inference</span>
orig_img_batch = orig_img_tensor.unsqueeze(<span class="syntax-number">0</span>)

<span class="syntax-comment"># Attach device to the image and the model</span>
orig_img_batch = orig_img_batch.to(device)
model = model.to(device)
</code><button class="codecopy_copy" data-codecopy="3">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="4.-load-imagenet-classes" tabindex="-1"><a href="#4.-load-imagenet-classes" class="header-anchor">4. Load ImageNet Classes</a></h3>
<p>We use the ImageNet classes. The labels will be used for checking which label the original image and adversarial images are classfied by the model.</p>
<pre class="codecopy_pre codecopy_pre_4"><code class="language-python hljs">wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt
</code><button class="codecopy_copy" data-codecopy="4">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>Then read this text file and assign to labels.</p>
<pre class="codecopy_pre codecopy_pre_5"><code class="language-python hljs"><span class="syntax-keyword">with</span> <span class="syntax-built_in">open</span>(<span class="syntax-string">"imagenet_classes.txt"</span>, <span class="syntax-string">"r"</span>) <span class="syntax-keyword">as</span> f:
  labels = [s.strip() <span class="syntax-keyword">for</span> s <span class="syntax-keyword">in</span> f.readlines()]
</code><button class="codecopy_copy" data-codecopy="5">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="5.-initial-prediction" tabindex="-1"><a href="#5.-initial-prediction" class="header-anchor">5. Initial Prediction</a></h3>
<p>Before creating adversarial examples, we need to know the classes and probabilities of the original image by the ResNet model.</p>
<pre class="codecopy_pre codecopy_pre_6"><code class="language-python hljs">pred = model(orig_img_batch)
probs = F.softmax(pred[<span class="syntax-number">0</span>], dim=<span class="syntax-number">0</span>)
probs_top5, idx_top5 = torch.topk(probs, <span class="syntax-number">5</span>)
<span class="syntax-built_in">print</span>(<span class="syntax-string">"The top 5 labels of highly probabilies:"</span>)
<span class="syntax-keyword">for</span> i <span class="syntax-keyword">in</span> <span class="syntax-built_in">range</span>(probs_top5.size(<span class="syntax-number">0</span>)):
  <span class="syntax-built_in">print</span>(<span class="syntax-string">f"<span class="syntax-subst">{labels[idx_top5[i]]}</span>: <span class="syntax-subst">{probs_top5[i].item()*<span class="syntax-number">100</span>:<span class="syntax-number">.2</span>f}</span>%"</span>)

<span class="syntax-comment"># Extract the top probability and index (target) for use in the next sections</span>
target_prob = probs_top5[<span class="syntax-number">0</span>]
target_idx = idx_top5[<span class="syntax-number">0</span>]
</code><button class="codecopy_copy" data-codecopy="6">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>The top5 labels/accuracies should be such as below.</p>
<pre class="codecopy_pre codecopy_pre_7"><code class="language-txt hljs language-plaintext">The top 5 labels of highly probabilies:
Samoyed: 87.33%
Pomeranian: 3.03%
white wolf: 1.97%
keeshond: 1.11%
Eskimo dog: 0.92%
</code><button class="codecopy_copy" data-codecopy="7">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>As we imagine, the <strong>ResNet</strong> model predicted the original image as <strong><code>Samoyed</code></strong> with <strong><code>87.33%</code></strong> accuracy.</p>
<h3 id="6.-define-function-to-denormalize" tabindex="-1"><a href="#6.-define-function-to-denormalize" class="header-anchor">6. Define Function to Denormalize</a></h3>
<p>Create a function to denormalize an input image. Since the original image must be denormalized before FGSM process, this function is used to do that.</p>
<pre class="codecopy_pre codecopy_pre_8"><code class="language-python hljs"><span class="syntax-keyword">def</span> <span class="syntax-title function_">denorm</span>(<span class="syntax-params">batch, mean=[<span class="syntax-number">0.485</span>, <span class="syntax-number">0.456</span>, <span class="syntax-number">0.406</span>], std=[<span class="syntax-number">0.229</span>, <span class="syntax-number">0.224</span>, <span class="syntax-number">0.225</span>]</span>):
  <span class="syntax-keyword">if</span> <span class="syntax-built_in">isinstance</span>(mean, <span class="syntax-built_in">list</span>):
    mean = torch.tensor(mean).to(device)
  <span class="syntax-keyword">if</span> <span class="syntax-built_in">isinstance</span>(std, <span class="syntax-built_in">list</span>):
    std = torch.tensor(std).to(device)
  <span class="syntax-keyword">return</span> batch * std.view(<span class="syntax-number">1</span>, -<span class="syntax-number">1</span>, <span class="syntax-number">1</span>, <span class="syntax-number">1</span>) + mean.view(<span class="syntax-number">1</span>, -<span class="syntax-number">1</span>, <span class="syntax-number">1</span>, <span class="syntax-number">1</span>)
</code><button class="codecopy_copy" data-codecopy="8">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="7.-calculate-perturbations" tabindex="-1"><a href="#7.-calculate-perturbations" class="header-anchor">7. Calculate Perturbations</a></h3>
<p>This process is the main role of the Adversarial Attack.<br>
It calculates the sign of the backpropagated gradients. It will be used for adjusting the input data to maximize the loss value in the next section.</p>
<pre class="codecopy_pre codecopy_pre_9"><code class="language-python hljs"><span class="syntax-keyword">def</span> <span class="syntax-title function_">calc_perturbations</span>(<span class="syntax-params">image, target</span>):
  image.requires_grad = <span class="syntax-literal">True</span>

  <span class="syntax-comment"># Predict the original image</span>
  pred = model(image)

  loss = F.nll_loss(pred, target)
  model.zero_grad()
  loss.backward()

  gradient = image.grad.data
  signed_grad = gradient.sign()
  <span class="syntax-keyword">return</span> signed_grad

perturbations = calc_perturbations(orig_img_batch, torch.tensor([target_idx]))
</code><button class="codecopy_copy" data-codecopy="9">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="8.-start-creating-adversarial-examples" tabindex="-1"><a href="#8.-start-creating-adversarial-examples" class="header-anchor">8. Start Creating Adversarial Examples</a></h3>
<p>Now generate adversarial exampels by each epsilon.<br>
The adversarial image is generated by adding the multiply of epsilong and perturbations to the original image data.<br>
Generally, the higher the value of <strong>epsilon</strong>, the less accuracy of the prediction by the model.</p>
<pre class="codecopy_pre codecopy_pre_10"><code class="language-python hljs">epsilons = [<span class="syntax-number">0</span>, <span class="syntax-number">.01</span>, <span class="syntax-number">.05</span>, <span class="syntax-number">.1</span>, <span class="syntax-number">.2</span>]

adv_examples = []

<span class="syntax-keyword">for</span> eps <span class="syntax-keyword">in</span> epsilons:
  orig_img_batch_denorm = denorm(orig_img_batch)
  adv_img = orig_img_batch_denorm + eps * perturbations
  adv_img = torch.clamp(adv_img, <span class="syntax-number">0</span>, <span class="syntax-number">1</span>)

  <span class="syntax-comment"># Normalize the adversarial image</span>
  adv_img_norm = transforms.Normalize((<span class="syntax-number">0.485</span>, <span class="syntax-number">0.456</span>, <span class="syntax-number">0.406</span>), (<span class="syntax-number">0.229</span>, <span class="syntax-number">0.224</span>, <span class="syntax-number">0.225</span>))(adv_img)

  <span class="syntax-comment"># Predict the adversarial example</span>
  adv_pred = model(adv_img_norm)
  adv_probs = F.softmax(adv_pred[<span class="syntax-number">0</span>], dim=<span class="syntax-number">0</span>)
  adv_probs_top5, adv_idx_top5 = torch.topk(adv_probs, <span class="syntax-number">5</span>)
  <span class="syntax-built_in">print</span>(<span class="syntax-string">"-"</span>*<span class="syntax-number">28</span> + <span class="syntax-string">f"Eps <span class="syntax-subst">{eps}</span>"</span> + <span class="syntax-string">"-"</span>*<span class="syntax-number">28</span>)
  <span class="syntax-keyword">for</span> i <span class="syntax-keyword">in</span> <span class="syntax-built_in">range</span>(adv_probs_top5.size(<span class="syntax-number">0</span>)):
    <span class="syntax-built_in">print</span>(<span class="syntax-string">f"<span class="syntax-subst">{labels[adv_idx_top5[i]]}</span>: <span class="syntax-subst">{adv_probs_top5[i]*<span class="syntax-number">100</span>:<span class="syntax-number">.2</span>f}</span>%"</span>)
  <span class="syntax-built_in">print</span>()

  <span class="syntax-comment"># Make the adversarial example to the image to be saved</span>
  adv_ex = adv_img.squeeze().detach().cpu().numpy()

  adv_examples.append((labels[adv_idx_top5[<span class="syntax-number">0</span>]], adv_probs_top5[<span class="syntax-number">0</span>], adv_ex))
</code><button class="codecopy_copy" data-codecopy="10">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>The output should be such as below.</p>
<pre class="codecopy_pre codecopy_pre_11"><code class="language-txt hljs language-plaintext">----------------------------Eps 0----------------------------
Samoyed: 87.33%
Pomeranian: 3.03%
white wolf: 1.97%
keeshond: 1.11%
Eskimo dog: 0.92%

----------------------------Eps 0.01----------------------------
West Highland white terrier: 43.36%
Scotch terrier: 8.47%
wallaby: 7.29%
cairn: 4.53%
Angora: 1.87%

----------------------------Eps 0.05----------------------------
West Highland white terrier: 92.15%
cairn: 1.28%
Angora: 1.16%
Scotch terrier: 1.06%
Maltese dog: 0.66%

----------------------------Eps 0.1----------------------------
West Highland white terrier: 97.47%
Scotch terrier: 0.57%
cairn: 0.31%
Angora: 0.17%
Maltese dog: 0.15%

----------------------------Eps 0.2----------------------------
West Highland white terrier: 50.01%
white wolf: 12.23%
ice bear: 8.72%
Arctic fox: 3.96%
Samoyed: 2.19%
</code><button class="codecopy_copy" data-codecopy="11">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>We should notice that adversarial images were not classified as <strong><code>Samoyed</code></strong>, but the other labels such as <strong><code>West Highland white terrier</code></strong> after the <strong>epsilon 0.01</strong>.</p>
<p>In short, we succeeded to fool the model’s predictions by modifying the original image.</p>
<h3 id="9.-plot-the-result" tabindex="-1"><a href="#9.-plot-the-result" class="header-anchor">9. Plot the Result</a></h3>
<p>Although this section is optional, we can plot the result above.</p>
<pre class="codecopy_pre codecopy_pre_12"><code class="language-python hljs"><span class="syntax-keyword">import</span> matplotlib.pyplot <span class="syntax-keyword">as</span> plt

cnt = <span class="syntax-number">0</span>
plt.figure(figsize=(<span class="syntax-number">28</span>, <span class="syntax-number">10</span>))

<span class="syntax-keyword">for</span> i, eps <span class="syntax-keyword">in</span> <span class="syntax-built_in">enumerate</span>(epsilons):
  cnt += <span class="syntax-number">1</span>
  plt.subplot(<span class="syntax-number">1</span>, <span class="syntax-built_in">len</span>(adv_examples), cnt)
  plt.xticks([])
  plt.yticks([])
  label, prob, img = adv_examples[i]
  plt.title(<span class="syntax-string">f"Eps <span class="syntax-subst">{eps}</span>\nClass: <span class="syntax-subst">{label}</span>\nAccuracy: <span class="syntax-subst">{prob*<span class="syntax-number">100</span>:<span class="syntax-number">.2</span>f}</span>%"</span>, fontsize=<span class="syntax-number">14</span>)
  plt.imshow(img.T)
plt.show()
</code><button class="codecopy_copy" data-codecopy="12">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>We should see that the noise gets louder as the epsilon increases.<br>
However, from human eyes, these images are <strong><code>Samoyed</code></strong> no matter how you look at them.</p>
<h3 id="10.-save-the-adversarial-examples" tabindex="-1"><a href="#10.-save-the-adversarial-examples" class="header-anchor">10. Save the Adversarial Examples</a></h3>
<p>Finally, we save the generated adversarial images.<br>
Create new folder to store all adversarial images to be downloaded.</p>
<pre class="codecopy_pre codecopy_pre_13"><code class="language-bash hljs"><span class="syntax-built_in">mkdir</span> fake_dogs
</code><button class="codecopy_copy" data-codecopy="13">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>Now save the images. We can use them to fool <strong>ResNet</strong> models.</p>
<pre class="codecopy_pre codecopy_pre_14"><code class="language-python hljs"><span class="syntax-comment"># Save adversarial images</span>
<span class="syntax-keyword">from</span> torchvision.utils <span class="syntax-keyword">import</span> save_image

<span class="syntax-keyword">for</span> i, eps <span class="syntax-keyword">in</span> <span class="syntax-built_in">enumerate</span>(epsilons):
  label, prob, ex = adv_examples[i]
  ex_tensor = torch.from_numpy(ex).clone()
  save_image(ex_tensor, <span class="syntax-string">f"fake_dogs/fake_dog_eps<span class="syntax-subst">{eps}</span>.png"</span>)
</code><button class="codecopy_copy" data-codecopy="14">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<br>
<h2 id="create-adversarial-examples-against-mobilenetv2" tabindex="-1"><a href="#create-adversarial-examples-against-mobilenetv2" class="header-anchor">Create Adversarial Examples against MobileNetV2</a></h2>
<p>Reference: <a href="https://www.tensorflow.org/tutorials/generative/adversarial_fgsm">TensorFlow Docs</a></p>
<h3 id="1.-load-pretrained-model-(mobilenetv2)" tabindex="-1"><a href="#1.-load-pretrained-model-(mobilenetv2)" class="header-anchor">1. Load Pretrained Model (MobileNetV2)</a></h3>
<pre class="codecopy_pre codecopy_pre_15"><code class="language-python hljs"><span class="syntax-keyword">import</span> tensorflow <span class="syntax-keyword">as</span> tf

pretrained_model = tf.keras.applications.MobileNetV2(include_top=<span class="syntax-literal">True</span>, weights=<span class="syntax-string">'imagenet'</span>)
pretrained_model.trainable = <span class="syntax-literal">False</span>

<span class="syntax-comment"># ImageNet labels</span>
decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions
</code><button class="codecopy_copy" data-codecopy="15">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="2.-prepare-original-image" tabindex="-1"><a href="#2.-prepare-original-image" class="header-anchor">2. Prepare Original Image</a></h3>
<p>We create functions to preprocess image and get label at first.</p>
<pre class="codecopy_pre codecopy_pre_16"><code class="language-python hljs"><span class="syntax-comment"># Helper function to preprocess the image so that it can be inputted in MobileNetV2</span>
<span class="syntax-keyword">def</span> <span class="syntax-title function_">preprocess</span>(<span class="syntax-params">image</span>):
  image = tf.cast(image, tf.float32)
  image = tf.image.resize(image, (<span class="syntax-number">224</span>, <span class="syntax-number">224</span>))
  image = tf.keras.applications.mobilenet_v2.preprocess_input(image)
  image = image[<span class="syntax-literal">None</span>, ...]
  <span class="syntax-keyword">return</span> image

<span class="syntax-comment"># Helper function to extract labels from probability vector</span>
<span class="syntax-keyword">def</span> <span class="syntax-title function_">get_imagenet_label</span>(<span class="syntax-params">probs</span>):
	<span class="syntax-keyword">return</span> decode_predictions(probs, top=<span class="syntax-number">1</span>)[<span class="syntax-number">0</span>][<span class="syntax-number">0</span>]
</code><button class="codecopy_copy" data-codecopy="16">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>Then load the original image and preprocess it.</p>
<pre class="codecopy_pre codecopy_pre_17"><code class="language-python hljs">orig_image_path = tf.keras.utils.get_file(<span class="syntax-string">'YellowLabradorLooking_new.jpg'</span>, <span class="syntax-string">'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg'</span>)
orig_image_raw = tf.io.read_file(image_path)
orig_image = tf.image.decode_image(image_raw)

orig_image = preprocess(image)
orig_image_probs = pretrained_model.predict(image)
</code><button class="codecopy_copy" data-codecopy="17">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>To get the label of the image that the model predicted, execute the following code.</p>
<pre class="codecopy_pre codecopy_pre_18"><code class="language-python hljs">_, orig_image_class, orig_class_confidence = get_imagenet_label(orig_image_probs)

<span class="syntax-built_in">print</span>(<span class="syntax-string">f"class: <span class="syntax-subst">{orig_image_class}</span>"</span>)
<span class="syntax-built_in">print</span>(<span class="syntax-string">f"confidence: <span class="syntax-subst">{orig_class_confidence}</span>"</span>)

<span class="syntax-comment"># The output</span>
<span class="syntax-comment"># class: Labrador_retriever</span>
<span class="syntax-comment"># confidence: 0.418184757232666</span>
</code><button class="codecopy_copy" data-codecopy="18">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="3.-create-adversarial-image-with-fgsm" tabindex="-1"><a href="#3.-create-adversarial-image-with-fgsm" class="header-anchor">3. Create Adversarial Image with FGSM</a></h3>
<p>From this, we create the adversarial image to fool the MobileNetV2 model. The following code creates the perturbations to modify the original image.</p>
<pre class="codecopy_pre codecopy_pre_19"><code class="language-python hljs"><span class="syntax-comment"># Instantiate a function that computes the crossentropy loss between labels and predictions.</span>
loss_obj = tf.keras.losses.CategoricalCrossentropy()

<span class="syntax-keyword">def</span> <span class="syntax-title function_">create_adversarial_pattern</span>(<span class="syntax-params">input_image, input_label</span>):
	<span class="syntax-comment"># The gradient tape records the operations which are executed inside it.</span>
  <span class="syntax-keyword">with</span> tf.GradientTape() <span class="syntax-keyword">as</span> tape:
    tape.watch(input_image)
    prediction = pretrained_model(input_image)
    loss = loss_obj(input_label, prediction)

  <span class="syntax-comment"># Get the gradients of the loss w.r.t (with respect to) to the input image.</span>
  gradient = tape.gradient(loss, input_image)
  <span class="syntax-comment"># Get the sign of the gradients to create the perturbation.</span>
  signed_grad = tf.sign(gradient)
  <span class="syntax-keyword">return</span> signed_grad

<span class="syntax-comment"># The index of the label for labrador retriever</span>
target_label_idx = <span class="syntax-number">208</span>
orig_label = tf.one_hot(target_label_idx, orig_image_probs.shape[-<span class="syntax-number">1</span>])
orig_label = tf.reshape(orig_label, (<span class="syntax-number">1</span>, orig_image_probs.shape[-<span class="syntax-number">1</span>]))

perturbations = create_adversarial_pattern(orig_image, orig_label)
</code><button class="codecopy_copy" data-codecopy="19">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>Now create adversarial examples and predict the labels by the classification model while increasing epsilon.</p>
<pre class="codecopy_pre codecopy_pre_20"><code class="language-python hljs"><span class="syntax-comment"># Epsilons are error terms (very small numbers)</span>
epsilons = [<span class="syntax-number">0</span>, <span class="syntax-number">0.01</span>, <span class="syntax-number">0.1</span>, <span class="syntax-number">0.15</span>]

<span class="syntax-keyword">for</span> i, eps <span class="syntax-keyword">in</span> <span class="syntax-built_in">enumerate</span>(epsilons):
	adv_image = orig_image + eps*perturbations
	adv_image = tf.clip_by_value(adv_image, -<span class="syntax-number">1</span>, <span class="syntax-number">1</span>)
	<span class="syntax-comment"># Predict the label and the confidence for the adversarial image</span>
	_, label, confidence = get_imagenet_label(pretrained_model.predict(adv_image))
	<span class="syntax-built_in">print</span>(<span class="syntax-string">f"predicted label: <span class="syntax-subst">{label}</span>"</span>)
	<span class="syntax-built_in">print</span>(<span class="syntax-string">f"confidence: <span class="syntax-subst">{confidence*<span class="syntax-number">100</span>:<span class="syntax-number">.2</span>f}</span>%"</span>)
	<span class="syntax-built_in">print</span>(<span class="syntax-string">"-"</span>*<span class="syntax-number">128</span>)
</code><button class="codecopy_copy" data-codecopy="20">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>The outputs are something like below.</p>
<pre class="codecopy_pre codecopy_pre_21"><code class="language-txt hljs language-plaintext">1/1 [==============================] - 0s 25ms/step
predicted label: Labrador_retriever
confidence: 41.82%
--------------------------------------------------------------------------------------------------------------------------------
1/1 [==============================] - 0s 27ms/step
predicted label: Saluki
confidence: 13.08%
--------------------------------------------------------------------------------------------------------------------------------
1/1 [==============================] - 0s 24ms/step
predicted label: Weimaraner
confidence: 15.13%
--------------------------------------------------------------------------------------------------------------------------------
1/1 [==============================] - 0s 26ms/step
predicted label: Weimaraner
confidence: 16.58%
--------------------------------------------------------------------------------------------------------------------------------
</code><button class="codecopy_copy" data-codecopy="21">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>As above, the adversarial examples were predicted as different labels from the label that the original image was predicted (the original label is labrador retriever).<br>
To display the final adversarial image, execute the following code.</p>
<pre class="codecopy_pre codecopy_pre_22"><code class="language-python hljs"><span class="syntax-keyword">import</span> matplotlib.pyplot <span class="syntax-keyword">as</span> plt

plt.imshow(adv_image[<span class="syntax-number">0</span>])
</code><button class="codecopy_copy" data-codecopy="22">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<h3 id="4.-save%2Fload-the-adversarial-image" tabindex="-1"><a href="#4.-save%2Fload-the-adversarial-image" class="header-anchor">4. Save/Load the Adversarial Image</a></h3>
<p>We can save the generated adversarial image as below.</p>
<pre class="codecopy_pre codecopy_pre_23"><code class="language-python hljs">tf.keras.utils.save_img(<span class="syntax-string">"fake.png"</span>, adv_image[<span class="syntax-number">0</span>])
</code><button class="codecopy_copy" data-codecopy="23">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<p>To load this image, use Pillow.</p>
<pre class="codecopy_pre codecopy_pre_24"><code class="language-python hljs"><span class="syntax-keyword">from</span> PIL <span class="syntax-keyword">import</span> Image

fake_img = Image.<span class="syntax-built_in">open</span>(<span class="syntax-string">"fake.png"</span>)
fake_img
</code><button class="codecopy_copy" data-codecopy="24">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewbox="0 0 24 24" stroke-width="2.0" stroke="currentColor" class="w-6 h-6">
    <path stroke-linecap="round" stroke-linejoin="round" d="M15.75 17.25v3.375c0 .621-.504 1.125-1.125 1.125h-9.75a1.125 1.125 0 01-1.125-1.125V7.875c0-.621.504-1.125 1.125-1.125H6.75a9.06 9.06 0 011.5.124m7.5 10.376h3.375c.621 0 1.125-.504 1.125-1.125V11.25c0-4.46-3.243-8.161-7.5-8.876a9.06 9.06 0 00-1.5-.124H9.375c-.621 0-1.125.504-1.125 1.125v3.5m7.5 10.375H9.375a1.125 1.125 0 01-1.125-1.125v-9.25m12 6.625v-1.875a3.375 3.375 0 00-3.375-3.375h-1.5a1.125 1.125 0 01-1.125-1.125v-1.5a3.375 3.375 0 00-3.375-3.375H9.75"></path>
</svg><span class="codecopy_result">Copied!</span></button></pre>
<br>

            </div>
            <div class="my-6 p-4">
                <h2 class="text-3xl font-bold border-b-[1px] border-pink-dark">References</h2>
                <ul class="my-6 ml-4 list-disc">
                    
                        <li class="my-2">
                            <a href="https://arxiv.org/abs/1412.6572" class="text-base text-pink-light hover:brightness-200">https://arxiv.org/abs/1412.6572</a>
                        </li>
                    
                        <li class="my-2">
                            <a href="https://arxiv.org/abs/1810.00069" class="text-base text-pink-light hover:brightness-200">https://arxiv.org/abs/1810.00069</a>
                        </li>
                    
                        <li class="my-2">
                            <a href="https://arxiv.org/abs/1804.00097" class="text-base text-pink-light hover:brightness-200">https://arxiv.org/abs/1804.00097</a>
                        </li>
                    
                        <li class="my-2">
                            <a href="https://tcode2k16.github.io/blog/posts/picoctf-2018-writeup/general-skills/#solution-20" class="text-base text-pink-light hover:brightness-200">https://tcode2k16.github.io/blog/posts/picoctf-2018-writeup/general-skills/#solution-20</a>
                        </li>
                    
                </ul>
            </div>
        </article>
    </div>

    
    <div id="exploit-rightside-inner" class="hidden sm:block w-full h-screen overflow-y-auto px-2 py-3">
        <div class="">
            <div class="">
                <h4 class="mb-4 border-b-2 border-slate-light text-base text-white opacity-60">Tools by Muhammd</h4>
                <div class="my-4">
                    <div class="my-2">
                        <a href="https://github.com/muhammd/muhammd" target="_blank" rel="noopener noreferrer" class="text-base text-pink-light hover:brightness-200">
                            RedTeam Repos
                        </a>
                        <p class="text-sm">Automatic PenTest Scripts</p>
                    </div>
                    <div class="my-2">
                        <a href="https://github.com/muhammd/autorecon" target="_blank" rel="noopener noreferrer" class="text-base text-pink-light hover:brightness-200">
                            AutoRecon
                        </a>
                        <p class="text-sm">Auto reconnaissance CLI.</p>
                    </div>
                    <div class="my-2">
                        <a href="https://PenTest.Tools/" target="_blank" rel="noopener noreferrer" class="text-base text-pink-light hover:brightness-200">
                            PenTest Tools 
                        </a>
                        <p class="text-sm">PenTest Tools</p>
                    </div>
                </div>
            </div>
            <div class="mt-8">
                
                <script async="" type="text/javascript" src="//cdn.carbonads.com/carbon.js?serve=CWYDE53L&amp;placement=exploit-noteshdksorg" id="_carbonads_js">
                </script>
            </div>
        </div>
    </div>
    
</div>
        </main>

        <footer class="w-full p-8 z-100 bg-navy-light">
            <div class="w-full flex flex-col items-center space-y-2 text-base">
                <div class="flex items-center justify-center space-x-2">
                    <a href="/disclaimer" class="text-pink-light hover:brightness-200">Disclaimer</a>
                    <a href="/privacy-policy" class="text-pink-light hover:brightness-200">Privacy Policy</a>
                </div>
                <hr class="w-[260px] white opacity-30">
                <div class="flex items-center space-x-2">
                    <a href="https://github.com/muhammd/" target="_blank" rel="noopener noreferrer" class="text-pink-light hover:brightness-200">
                        GitHub
                    </a>            
                    <a href="https://twitter.com/muhammdhaidari" target="_blank" rel="noopener noreferrer" class="text-pink-light hover:brightness-200">
                        Twitter
                    </a>
                </div>
            </div>
        </footer>
    

</body></html>